{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "**How to submit.** For this homework, submit this `.ipynb` file with your answers.\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Importance of data (4 points)\n",
    "\n",
    "As we have discussed before, language is very diverse. Right now, machine translation models can work quite well in limited settings, but a single model is rarely good at dealing with diverse kinds of texts. Therefore, it is very important what data the model is trained on and what data it is applied to. In this task, you will verify that a model trained on a particular type of texts will perform worse when it has to translate texts of a different domain.\n",
    "\n",
    "**Subtask 1 (2.5 points).** You have two ET$\\rightarrow$EN Sockeye [models](https://owncloud.ut.ee/owncloud/index.php/s/bMCsePCQ2ZWoQAM): one was trained on a corpus of legislative documents (DGT), and the other on a corpus of movie and TV subtitles (OpenSubtitles). You also have two test sets, one from the DGT corpus and one from OpenSubtitles. Preprocess the sets (preprocessing models are also provided), translate each of the test sets with each of the models, postprocess. Measure BLEU score for each of the four translations (using sacreBLEU, in the same way you did in Homework 3). Report the results, explain what you see.\n",
    "\n",
    "**Hint.** Translation will be faster if you increase batch size, e.g. `--batch-size 32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask 2 (1.5 points).** You can probably observe that the OpenSubtitles model performs much worse on the OpenSubtitles test set than the DGT model does on the DGT test set. The models were trained in exactly the same way: the preprocessing pipeline is the same, the architecture and number of parameters is the same, the number of training sentence pairs is the same, the number of epochs is the same, only the corpora the sentences come from are different. How would you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Beam search (4 points)\n",
    "\n",
    "During inference (translation), the model generates a probability distribution over the vocabulary at each step. For each word in the sentence that distribution depends on the previously generated words. To choose the most promising next step, beam search is used.\n",
    "\n",
    "**Subtask 1 (1.5 points).** Google around and explain (in  4-5 sentences) what is beam search and why we need it for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask 2 (2.5 points).** Beam size is a parameter that does not depend on the model and can be set during translation. The default beam size Sockeye uses is 5. Experiment with the test set that you used in Homework 3. Translate the test set with your baseline model using different beam sizes (for example, 1, 3, 5 and 10). Don't forget to postrocess your translations. Plot BLEU and translation time with different beam sizes (Sockeye reports total time, seconds per sentence and sentences per second in its translation log). Describe what you see.\n",
    "\n",
    "If you do not have a well trained baseline model right now (if your model showed BLEU score less than 10 on the test set in Homework 3), use one of the models provided for Task 1.\n",
    "\n",
    "**Hint.** Do all translations from one SLURM script, then all translations will be done on the same node. Definitely use the same batch size for all translations. This will ensure that the only thing that brings in the difference is the beam size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Improving the baseline (2 points)\n",
    "\n",
    "In Homework 3 you summarized what kinds of mistakes your baseline model tends to make. In this task, you should come up with some ideas how to fix those mistakes.\n",
    "\n",
    "**Note.** If your model showed BLEU less than 10 on the test set from the previous homework, it means that something was wrong with the training. You should find what went wrong, retrain your model properly and redo the analysis. Otherwise it makes no sense for you to do this task. \n",
    "\n",
    "**Subtask 1 (2 points).** Propose some modifications to your model that you think could fix the typical mistakes your baseline model makes. Those modifications can be changes to the architecture, changes to the training data or preprocessing pipeline, or other tricks you can come up with. (Of course, one thing you should do to get a better model is leave it to train for a longer time, but this does not count as a modification.) Explain why you think your proposed modifications could fix the particular mistakes you mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
