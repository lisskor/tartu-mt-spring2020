{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to submit.** For this homework, submit this `.ipynb` file with your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will explore MT quality metrics and work on evaluating the Estonian$\\rightarrow$English model that you trained in homework 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tricking BLEU (2 points)\n",
    "\n",
    "The most popular automatic metric for evaluating machine translation quality is BLEU (bilingual evaluation understudy). It measures how close a translation is to a reference (\"ground truth\") translation produced by a human. We put \"ground truth\" in quotes, because, unlike with, say, classification, in machine translation there is no single correct answer. Several different translations can all be perfectly correct, while having very different wording.\n",
    "\n",
    "BLEU claims high correlation with human judgements of how good a translation is. However, if we are looking at a BLEU score for one sentence, and not an average score over many sentences, that number can be misleading.\n",
    "\n",
    "**Subtask 1 (1 point)**. Try to come up with examples of translations that can fool BLEU. (If you are unsure how BLEU works, check out the practice session materials or google around.) Bring an example of a sentence in some language you know, a good translation of this sentence into English, and a bad translation into English, which would have a decent BLEU score with the good translation as reference. (Please also explain what is happening in your non-English sentence, what it means and why the bad translation is bad.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask 2 (1 point).** Now do the same, but the other way around: come up with a sentence in your language, a good reference translation of that sentence into English, and another translation which is also good, but would have a low BLEU score when compared to the first translation. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Other metrics (1.5 points)\n",
    "\n",
    "BLEU is the most popular automatic metric, but it has its disadvantages, which other metrics try to overcome. In this task we will look at some of these other metrics.\n",
    "\n",
    "**Subtask 1 (1.5 points).** Explain the main idea behind three MT quality metrics and how they are different from BLEU. No formulas needed, 1-2 sentences about each metric is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIST:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METEOR:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Calculate your model's BLEU (1.5 points)\n",
    "\n",
    "**Subtask 1 (1.5 points).** Download the test set from [Owncloud](https://owncloud.ut.ee/owncloud/index.php/s/FpcDPYSr6FipqsT). There are two files. One of them (`test-src.et`) contains the Estonian side of the test set. Preprocess this set, translate it with your model from the previous homework, and postprocess the result. The second file (`test-ref.en`) contains a human translation of the set. Compare your translation to this reference translation by calculating the BLEU score with `sacreBLEU`. (If you are doing this on Rocket, do `module load python-3.6.3`, and then you can use the existing `sacrebleu` in the base environment. Locally, you will need to install sacreBLEU: `pip install sacrebleu`.)\n",
    "\n",
    "Use sacreBLEU in the following way:\n",
    "\n",
    "`cat hypothesis.en | sacrebleu reference_translation.en`\n",
    "\n",
    "Report the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Manual analysis (5 points)\n",
    "\n",
    "Even though automatic metrics are widely used to evaluate machine translation quality, they cannot show what kinds of errors the models make. A number provided by an automatic metric is not enough to make informed decisions about how to improve your model. It is always important to have an idea of what exactly your model is doing right and wrong.\n",
    "\n",
    "That is why, in this task, you will manually evaluate your model's performance on the test set that you translated in task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask 1 (4 points).** Analyse 30 sentences from the translated test set. For each of the 30, report:\n",
    "\n",
    "1. Sentence ID (line number)\n",
    "2. Source sentence (in Estonian)\n",
    "3. Reference translation\n",
    "4. Machine translation (by your model)\n",
    "5. Description of errors in the translation. You may use any system that seems reasonable to you. For instance, you could classify errors as \"word order errors\", \"untranslated words in source\", etc. A description of a sentence can be something like \"it tried to represent meaning, but made grammatical errors\" or \"hypothesis is fluent, but does not represent meaning correctly\".\n",
    "\n",
    "**Hint.** A convenient tool for comparing translations to references: [https://www.letsmt.eu/Bleu.aspx](https://www.letsmt.eu/Bleu.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask 2 (1 point).** Can you see any patterns and typical errors? Summarize your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
